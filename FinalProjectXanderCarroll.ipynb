{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darwin Plays Super Mario Bros\n",
    "### *Training A Reward-Based Machine Learning Model to Play Classic Video Games*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Xander Carroll    \n",
    "**Course:** Physics 5680, Autumn 2025  \n",
    "**Date:** December 16, 2025\n",
    "\n",
    "**Project Repository:** [Link](https://github.com/Xander-Carroll/PHYSICS5680-Neural-Network-Final)\n",
    "&nbsp;\n",
    "\n",
    "<small>*I used GPT-5 mini to help craft several paragraphs of text throughout this notebook.*</small>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic video games offer a controlled and well-understood environment for experimenting with machine learning, providing clear objectives and measurable rewards without the complexity of modern games. This project will explore training a reward-based neural network to play Super Mario Bros on the Nintendo Entertainment System (NES). The game state will be extracted using an NES emulator and encoded into a representation suitable for a neural network. The network will then be trained to maximize in-game rewards such as level progress and the level timer. It is anticipated that this approach will demonstrate how simple, reward-driven models can learn effective strategies in constrained environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Description\n",
    "Developing artificial agents that can learn to play video games has long been a benchmark problem in machine learning and artificial intelligence. While modern games offer highly complex environments, classic games like Super Mario Bros for the Nintendo Entertainment System (NES) provide simpler, well-defined environments where objectives, rewards, and state representations are clear. The problem we aim to solve is training a reward-based neural network to play Super Mario Bros, using in-game feedback to guide learning. This requires designing a system that can interpret the game state, map it into a format suitable for a neural network, and optimize agent behavior to choose the controller inputs that will maximize cumulative rewards.\n",
    "\n",
    "### Motivation\n",
    "This project advances research in reinforcement learning by testing how reward-based neural networks can learn behavior in simple, well-defined environments. Classic games like Super Mario Bros provide an ideal platform for such experiments. It is easy to simulate, has clear objectives, and still requires sequential decision-making and adaptation. By evolving a neural network to play the game using only in-game rewards as feedback, this project explores how complex behavior can emerge from simple fitness functions.\n",
    "\n",
    "### Background\n",
    "Our primary environment is the NES version of [Super Mario Bros](https://en.wikipedia.org/wiki/Super_Mario_Bros).$`^1`$ This game is one of the most popular from its era, has been reverse engineered, and is very well documented by the community. Super Mario Bros is a side-scrolling platformer. The player's goal is to make forward progress, eventually reaching the end of the level while avoiding hazards. The [BizHawk](https://tasvideos.org/Bizhawk) NES emulator will be used to provide a programmatic interface to read the game's memory and feed controller inputs to the game in real-time.$`^2`$ This will allow the neural network to \"see\" and \"interact\" with the game. The problem is framed as a reward-based learning task, where the agent receives feedback proportional to in-game progress, creating a natural fitness function for optimization.\n",
    "\n",
    "### Inputs and Outputs\n",
    "Input(s): The algorithm will recieve the current game state from the BizHawk emulator. This will include a map of the level, with terrain layout, enemy locations, and the player's current progress. This information will be encoded into a representation suitable for neural network processing.\n",
    "\n",
    "Output(s): The network will produce a set of controller inputs (e.g., hold left, press button A) to be executed in the game environment. Eventually, we expect these controller outputs to maximize the fitness function (player's foward progress in the level). \n",
    "\n",
    "### Project Goals\n",
    "\n",
    "1. **Primary Goal:**\n",
    "Train a fitness-based neural network to autonomously play Super Mario Bros and achieve measurable progress through one or more levels. The primary metric is maximizing cumulative in-game reward, including level completion.\n",
    "\n",
    "1. **Stretch Goal:**\n",
    "Extend the model to generalize across multiple levels or similar games. This will include testing whether a network trained on one level can adapt to unseen layouts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source 1:**\n",
    "\n",
    "*Stanley, Kenneth O., and Risto Miikkulainen. \"Evolving neural networks through augmenting topologies.\" Evolutionary computation 10.2 (2002): 99-127.*\n",
    "\n",
    "This is one of the early papers describing NEAT (NeuroEvolution of Augmenting Topologies). The NEAT algorithim not only adjusts the weights of connections in neural networks, but also their topologies (the nodes and connections in the network). The network starts with no hidden nodes, and over time will evolve to add them.\n",
    "\n",
    "**Source 2:**\n",
    "\n",
    "*Sethbling. “MarI/O - Machine Learning for Video Games.” YouTube, YouTube, www.youtube.com/watch?v=qv6UVOQ0F44. Accessed 13 Nov. 2025.*\n",
    "\n",
    "This project directly inspired my own work. It applies NEAT to train an artificial neural network to play Super Mario World for the Super Nintendo Entertainment System. The system evaluates agents based on a fitness function that rewards forward progress and then breeds the best performers to produce increasingly complex behavior over successive generations.\n",
    "\n",
    "**Source 3:**\n",
    "\n",
    "*Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\" arXiv preprint arXiv:1312.5602 (2013).*\n",
    "\n",
    "This paper presents the Deep Q‑Network (DQN) approach, where a convolutional neural network is trained with Q‑learning from raw pixel input to play multiple Atari 2600 games. \n",
    "\n",
    "**Source 4:**\n",
    "\n",
    "*Chrispresso. “Ai Learns to Play Super Mario Bros Using a Genetic Algorithm and Neural Network.” Chrispresso, 14 Mar. 2020, chrispresso.github.io/AI_Learns_To_Play_SMB_Using_GA_And_NN. *\n",
    "\n",
    "This project uses Deep Q learning to evolve a neural network capable of playing \"Super Mario Bros\" on the NES. Instead of training through gradient descent, the model’s weights are optimized using a fitness score that rewards progress and survival.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Strengths and Weaknesses:**\n",
    "\n",
    "I now have two sources using the NEAT algorithim, and two sources using the Q-Learning algorthim. In both cases, I have a paper detailing the aproach, and a project where the technique is applied to a Mario game.\n",
    "\n",
    "The NEAT-based approaches are simple and can be implemented from scratch, while the Q-Learning approaches will need to be implemented using a library like Tensorflow. However, deep learning libraries tend to have abundant documentation for use in python projects.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**The Two Approaches and State of the Art:**\n",
    "\n",
    "Modern state-of-the-art videogame playing agents tend to use deep learning methods such as DQN, which reliably learn from raw pixels using stable training techniques. Neuroevolution remains popular for small environments and hobbyist projects, especially when reward shaping or topology discovery is important. The related Mario projects are not state-of-the-art, and show both styles applied in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ethical Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this project uses a self-contained video game environment, the ethical concerns are relatively minimal compared to many real-world machine learning applications. The data used to train the model comes entirely from the \"Super Mario Bros\" game itself. This means there is no personal, demographic, or sensitive data involved, and thus no risk of violating data privacy or introducing human-related bias. The training process is purely synthetic, relying on game-generated feedback rather than external datasets.\n",
    "\n",
    "Potential misuse is also limited, as the model is designed for research and educational purposes. However, as with many AI techniques, the underlying algorithms (Deep Q-Learning and NEAT) could theoretically be adapted for automation or control systems in other domains, where ethical considerations such as fairness, accountability, or safety would become more significant.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Project Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Key Libraries\n",
    "\n",
    "Below is a brief description of the primary Python packages that will be used in this project.\n",
    "\n",
    "  * **pandas:** Used for data manipulation and analysis. It provides powerful data structures, like the DataFrame, for handling and exploring structured data.\n",
    "  * **numpy:** The fundamental package for scientific computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\n",
    "  * **matplotlib:** This library is for data visualization. Matplotlib is a comprehensive library for creating static visualizations, while Seaborn provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "  * **TensorFlow:** An end-to-end open-source platform for machine learning, specializing in deep learning. It is used for building and training neural networks for tasks like image classification, natural language processing, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\Users\\xande\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\xande\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Import all necessary external libraries here\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, Model, optimizers\n",
      "File \u001b[1;32mc:\\Users\\xande\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32mc:\\Users\\xande\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     86\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\Users\\xande\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "# Import all necessary standard libraries here\n",
    "import os, csv, sys, math, socket\n",
    "\n",
    "# Import all necessary external libraries here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers, Model, optimizers\n",
    "\n",
    "# Configure plots for readability\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Version Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]\n",
      "numpy version: 2.1.3\n",
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# Print version information\n",
    "from numpy import __version__ as np_version\n",
    "from pandas import __version__ as pd_version\n",
    "from tensorflow import __version__ as tf_version\n",
    "from matplotlib import __version__ as mpl_version\n",
    "\n",
    "print(f\"python version: {sys.version}\")\n",
    "print(f\"numpy version: {np_version}\")\n",
    "print(f\"pandas version: {pd_version}\")\n",
    "print(f\"tensorflow version: {tf_version}\")\n",
    "print(f\"matplotlib version: {mpl_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Dataset\n",
    "\n",
    "**Description:**\n",
    "\n",
    "The dataset for this project is not a static collection, but a stream of real-time input vectors extracted from the NES game \"Super Mario Bros.\" as it runs inside the BizHawk emulator. Each sample corresponds to a single game frame and consists of a simple encoding for the state of the level.\n",
    "\n",
    "Because the game runs at 60 frames per second, the agent will generate tens or hundreds of thousands of frames through its own gameplay attempts, which serve as the training data. Because the agent continuously produces new states through exploration, the traditional split into training, validation, and test sets does not directly apply. Instead, the \"training\" set will be the ammount of frames the agent spends learning the game, while the \"testing\" set is given by releasing the final agent to play various levels.\n",
    "\n",
    "**Data Spesifics:**\n",
    "\n",
    "The data is very simple. Each frame, N blocks around the player are represented with a state vector. The vector encodes \"air\" as zeros, \"tiles\" as ones, and \"enemies\" as negative ones. Later, additional information might be included in this vector, such as mario's position, the level timer, or the score.\n",
    "\n",
    "**Preprocessing:**\n",
    "\n",
    "All preprocessing occurs inside a Lua script running in BizHawk. The script reads specific memory addresses each frame (e.g., Mario’s position, tile map contents, and enemy locations) using the emulator’s API. These values are packaged into a compact vector representation of the current world state. An example of this encoding with N=6 (6 tiles of \"vision\" in any direction from the player) are shown below. In the second case, a red box is drawn with N=6 tiles around the player.\n",
    "\n",
    "The Lua script then transmits each state vector to a Python backend via TCP, where it will be fed directly into the Q-learning model.\n",
    "\n",
    "**Source:**\n",
    "\n",
    "The dataset originates entirely from gameplay in Super Mario Bros (Nintendo, 1985) executed within the BizHawk open-source emulator. Data is collected from RAM while the game is being played.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Level 1 - Still Frame             |  Level 1 - Vector Encoding\n",
    ":-------------------------:|:-------------------------:\n",
    "![](img/level-1.png)  |  ![](img/matrix-1.png)\n",
    "\n",
    "\n",
    "Level 2 - Still Frame             |  Level 2 - Vector Encoding\n",
    ":-------------------------:|:-------------------------:\n",
    "![](img/level-2.png)  |  ![](img/matrix-2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Extraction Script:**\n",
    "\n",
    "The following *lua* script will be used with the *BizHawk* emulator to extract data from the game each frame. It will then send the data to a TCP server. This will allow us to interact with the data using python. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---- EDITABLE PARAMETERS (you can change these)\n",
    "\n",
    "-- The port of the python server that will do the neural network computations.\n",
    "local PORT = 2022\n",
    "\n",
    "-- How many tiles the player can \"see\" in any direction.\n",
    "local VISION_SIZE = 6;\n",
    "\n",
    "-- How many frames between processing the data.\n",
    "local SEND_INTERVAL = 5;\n",
    "\n",
    "\n",
    "\n",
    "---- LIBRARY IMPORTS\n",
    "local socket = require('socket')\n",
    "\n",
    "\n",
    "\n",
    "---- CONSTANT VARIABLES\n",
    "\n",
    "-- Important RAM addresses.\n",
    "local ADDRESS_TILES = 0x0500;       -- Array<int>*  : The tile array/matrix\n",
    "local ADDRESS_PAGE = 0x006D;        -- int*         : The current page within the level\n",
    "local ADDRESS_HPOS = 0x0086;        -- int*         : The players horizontal position (in the current page)\n",
    "local ADDRESS_VPOS = 0x03B8;        -- int*         : The players vertical position\n",
    "local ADDRESS_SPRITES = 0x000F;     -- Array<int>*  : The sprite array\n",
    "local ADDRESS_EPAGE = 0x006E;       -- Array<int>*  : The page of the sprites within the level\n",
    "local ADDRESS_EHPOS = 0x0087;       -- Array<int>*  : The horizontal position of the sprites\n",
    "local ADDRESS_EVPOS = 0x00CF;       -- Array<int>*  : The vertical position of the sprites\n",
    "local ADDRESS_PSTATE = 0x000E;      -- int*         : The player's state (normal, dying, climbing, etc)\n",
    "\n",
    "-- How wide and tall a RAM page is in tiles.\n",
    "local PAGE_HEIGHT = 13;\n",
    "local PAGE_WIDTH = 16;\n",
    "\n",
    "-- The size of each sprite/tile in pixels.\n",
    "local SPRITE_WIDTH = 8;\n",
    "local TILE_WIDTH = 16;\n",
    "\n",
    "-- The tile values which will be considered \"air\".\n",
    "local AIR_VALUES = {0x00, 0x24, 0x25, 0xC2, 0XC3, 0xC5};\n",
    "\n",
    "-- The state values which will be considered a \"reset\".\n",
    "local RESET_VALUES = {0x04, 0x05, 0x06, 0x0b};\n",
    "\n",
    "\n",
    "---- SCRIPT VARIABLES\n",
    "\n",
    "-- table<string, boolean> : Keys that are currently being pressed.\n",
    "local keyInputs;\n",
    "\n",
    "-- int : The number of inputs to the network.\n",
    "local inputSize = (VISION_SIZE*2+1)^2;\n",
    "\n",
    "\n",
    "\n",
    "---- UTILITY FUNCTIONS\n",
    "\n",
    "-- Returns true if the table contains the value, false otherwise.\n",
    "function tableContains(table, value)\n",
    "    for i = 1, #table do\n",
    "        if table[i] == value then\n",
    "            return true\n",
    "        end\n",
    "    end\n",
    "  return false\n",
    "end\n",
    "\n",
    "-- Returns a table with all the keys listed in the string as True and False otherwise.\n",
    "function createControllerMap(outputString)\n",
    "    -- An output table with every key marked as \"False\".\n",
    "    local outputTable = {};\n",
    "\n",
    "    -- For each word in the output string.\n",
    "    for key in outputString:gmatch(\"%S+\") do\n",
    "        -- Until the \"END\" keyword.\n",
    "        if key ~= \"END\" then\n",
    "            -- Mark that key as \"True\"\n",
    "            outputTable[\"P1 \" .. key] = true;\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return outputTable;\n",
    "end\n",
    "\n",
    "-- Get the player's position (top left of sprite) in world coordinates.\n",
    "function getPlayerPosition()\n",
    "    -- Calculated as (screen position + page_width*number_of_pages_cleared).\n",
    "    local playerX = memory.readbyte(ADDRESS_HPOS) + (PAGE_WIDTH*TILE_WIDTH)*memory.readbyte(ADDRESS_PAGE);\n",
    "    \n",
    "    -- Calcualted as (screen position - mario_height).\n",
    "    local playerY = memory.readbyte(ADDRESS_VPOS) - TILE_WIDTH;\n",
    "\n",
    "    -- Return the player's position.\n",
    "    return playerX, playerY;\n",
    "end\n",
    "\n",
    "-- Check the tile (dx, dy) tiles away from mario. Returns 1 if it is a block, 0 if it is air.\n",
    "function getTile(playerX, playerY, dx, dy)\n",
    "    -- Get the (x,y) position of the tile in world coordinates.\n",
    "    local x = playerX + SPRITE_WIDTH + TILE_WIDTH*dx;\n",
    "    local y = playerY + TILE_WIDTH*dy;\n",
    "\n",
    "    -- Get the (x,y) index of the tile in the current page.\n",
    "    local xIndex = math.floor((x % (PAGE_WIDTH*TILE_WIDTH)) / TILE_WIDTH);\n",
    "    local yIndex = math.floor(y / TILE_WIDTH);  \n",
    "\n",
    "    -- If the tile is outside of the screen, return 0.\n",
    "    if xIndex >= PAGE_WIDTH or x < 0 then return 0; end\n",
    "    if yIndex >= PAGE_HEIGHT or y < 0 then return 0; end\n",
    "\n",
    "    -- The tile could be in one of two different \"pages\" in RAM.\n",
    "    local page = math.floor(x / (PAGE_WIDTH*TILE_WIDTH)) % 2;\n",
    "\n",
    "    -- Using the tile index and page index, find the address of the tile in RAM.\n",
    "    -- Calcualted as (array_address + page_offset + y_row + x_col)\n",
    "    local address = ADDRESS_TILES + page*PAGE_HEIGHT*PAGE_WIDTH + yIndex*PAGE_WIDTH + xIndex;\n",
    "    \n",
    "    -- Decide if there is a block at that address.\n",
    "    local tileType = memory.readbyte(address);\n",
    "    local isAir = tableContains(AIR_VALUES, tileType);\n",
    "    \n",
    "    -- Return the result.\n",
    "    if isAir then return 0; end\n",
    "    return 1;\n",
    "end\n",
    "\n",
    "-- Return all of the sprite objects (enemies, powerups, moving objects, etc).\n",
    "function getSprites()\n",
    "    local sprites = {};\n",
    "\n",
    "    -- Check every sprite slot.\n",
    "    for slot=0,4 do\n",
    "        -- If something is in the slot, add it to the sprite list.\n",
    "        if memory.readbyte(ADDRESS_SPRITES+slot) == 1 then\n",
    "            local spriteX = memory.readbyte(ADDRESS_EHPOS + slot) + (PAGE_WIDTH*TILE_WIDTH)*memory.readbyte(ADDRESS_EPAGE + slot);\n",
    "            local spriteY = memory.readbyte(ADDRESS_EVPOS + slot) - SPRITE_WIDTH - TILE_WIDTH;\n",
    "\n",
    "            sprites[#sprites+1] = {[\"x\"]=spriteX, [\"y\"]=spriteY};\n",
    "        end\n",
    "    end\n",
    "\n",
    "    -- Return the filled list.\n",
    "    return sprites;\n",
    "end\n",
    "\n",
    "-- Return the input vector that will be fead to the neural network.\n",
    "function getInputs(playerX, playerY)\n",
    "    local inputs = {};\n",
    "\n",
    "    -- Get all of the currently active sprites.\n",
    "    local sprites = getSprites();\n",
    "\n",
    "    -- Build the input array.\n",
    "    for dy=-VISION_SIZE,VISION_SIZE do\n",
    "        for dx=-VISION_SIZE,VISION_SIZE do\n",
    "            -- The tile (0 or 1) will be used as input.\n",
    "            inputs[#inputs+1] = getTile(playerX, playerY, dx,dy);\n",
    "\n",
    "            -- Unless a sprite is in the space, and then -1 will be used.\n",
    "            for i=1,#sprites do\n",
    "                -- Check how far from the space the sprite is.\n",
    "                distx = math.abs(sprites[i][\"x\"] - (playerX + dx*TILE_WIDTH));\n",
    "                disty = math.abs(sprites[i][\"y\"] - (playerY + dy*TILE_WIDTH));\n",
    "\n",
    "                -- If it is within half a tile, it is in the sapce.\n",
    "                if distx <= SPRITE_WIDTH and disty <= SPRITE_WIDTH then\n",
    "                    inputs[#inputs] = -1;\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    -- Return the inputs array.\n",
    "    return inputs;\n",
    "end\n",
    "\n",
    "-- Will send the input vector to the python server.\n",
    "function sendInputs(sock)\n",
    "    -- Get the player's position in world coordinates.\n",
    "    local playerX, playerY = getPlayerPosition();\n",
    "\n",
    "    -- Get the network inputs.\n",
    "    local inputs = getInputs(playerX, playerY);\n",
    "\n",
    "    -- Create a message string to send over TCP.\n",
    "    local message = \"\"\n",
    "\n",
    "    -- Add a variable indicating if the player has won (1) or died (2).\n",
    "    local playerState = (memory.readbyte(ADDRESS_PSTATE) == 0x04) and 1 or 0\n",
    "    playerState = (memory.readbyte(ADDRESS_PSTATE) == 0x06 or memory.readbyte(ADDRESS_PSTATE) == 0x0b) and 2 or playerState\n",
    "    message = message .. playerState .. \" \"\n",
    "\n",
    "    -- Add the player's horizontal position.\n",
    "    message = message .. playerX .. \" \"\n",
    "\n",
    "    for i=1,#inputs do\n",
    "        message = message .. inputs[i] .. \" \"\n",
    "    end\n",
    "    \n",
    "    -- Add a message terminator.\n",
    "    message = message .. \"END\"\n",
    "\n",
    "    -- Send the message.\n",
    "    sock:send(message);\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "---- CONNECT TO PYTHON SERVER\n",
    "\n",
    "-- Connect to the server.\n",
    "local sock = assert(socket.tcp());\n",
    "local success, err = sock:connect(\"localhost\", PORT);\n",
    "\n",
    "-- If there was an error, abort.\n",
    "if not success then\n",
    "    print(\"[NOTICE]: Could not connect to python server. Aborting.\");\n",
    "    return; \n",
    "end\n",
    "\n",
    "-- Set socket options.\n",
    "sock:settimeout(-1)\n",
    "sock:setoption(\"tcp-nodelay\", true)\n",
    "\n",
    "\n",
    "---- MAIN GAME LOOP\n",
    "\n",
    "savestate.loadslot(1); \n",
    "\n",
    "currentFrame = 0\n",
    "controller = {};\n",
    "\n",
    "while true do\n",
    "    currentFrame = currentFrame + 1;\n",
    "\n",
    "    -- Get the controller inputs every frame.\n",
    "    keyInputs = input.get()\n",
    "\n",
    "    -- The escape key stops the script.\n",
    "    if keyInputs[\"Escape\"] then break; end\n",
    "\n",
    "    -- If the player wins or dies we are going to process the frame, then reset.\n",
    "    local shouldReset = tableContains(RESET_VALUES, memory.readbyte(ADDRESS_PSTATE));\n",
    "\n",
    "    if currentFrame % SEND_INTERVAL == 0 or shouldReset then\n",
    "        -- Send the current network to the python server.\n",
    "        sendInputs(sock);\n",
    "\n",
    "        -- Wait for the server to finish processing.\n",
    "        local response, err = sock:receive();\n",
    "        if not response then\n",
    "            print(\"[NOTICE]: Python server disconnected. Aborting.\");\n",
    "            break; \n",
    "        end\n",
    "\n",
    "        -- Collect key outputs from the server and use them as joypad inputs.\n",
    "        controller = createControllerMap(response);\n",
    "    \n",
    "        -- Reset if the player has won or died.\n",
    "        if shouldReset then \n",
    "            savestate.loadslot(1); \n",
    "        end\n",
    "    end\n",
    "\n",
    "    -- Advance the frame.\n",
    "    joypad.set(controller)\n",
    "    emu.frameadvance();\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "---- GRACEFULLY CLOSE THE CONNECTION\n",
    "sock:close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Methods\n",
    "\n",
    "### 6.1 Deep Q-Learning (DQN)\n",
    "The basic idea of Deep-Q-Learning is to train an agent to choose actions that maximize long-term reward. Instead of manually programming rules for how Mario should behave, the algorithm learns by playing the game and gradually estimating the \"reward\" that will be generated by taking different actions in a specific state.\n",
    "\n",
    "This reward is quantified by the Q-function $Q(S,A)$. This function answers the question, *if the agent takes action A in state S, how much total reward should it expect?* Classic Q-Learning uses a lookup table, but for games like *Super Mario Bros.*, there are too many possible input states to build a table for them all. So we will use Deep Q-Learning which replaces the table with a neural network that *approximates* the Q-function, allowing the agent to generalize in situations that the programmer hasn't seen. The following image highlights these differences:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/q-learning-1.png\">\n",
    "\n",
    "<small><cite>Figure from: https://www.researchgate.net/figure/Q-Learning-vs-Deep-Q-Learning-Difference-in-the-agents-brain-In-Q-learning-the_fig4_382675650</cite>.$`^3`$</small>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "TensorFlow will handle the neural network that approximates this Q-function. On each frame, the model receives a state vector. It will then output a predicted Q-value for each action (each button on the controller). Actions with the highest predicted reward values will be selected. As the agent plays, it will store its past experiences, and use them to train the neural network. Many different kinds of neural networks can be used as a Q function, but convolutional neural networks (CNNs) are a common option. \n",
    "\n",
    "<br/>\n",
    "\n",
    "### 6.2 Convolutional Neural Network (CNN)\n",
    "While CNNs are not used in every DQN implementation, they provide a helpful way to understand how the algorithim typically processes game data.\n",
    "\n",
    "A CNN works by finding patterns in structured data, especially image vectors like the one extracted from *Super Mario Bros*. Instead of connecting every tile to every neuron, a CNN uses a sliding kernel that moves across the input in order to detect features of the image.\n",
    "\n",
    "In the broader DQN application, CNNs are used as a Q function. Each frame of the game can be processed by the CNN, and that network can be used to predict the outcomes of different actions. A loss function will then measure how correct that prediction was, and backpropogation will be used to update each parameter in the network.\n",
    "\n",
    "Because CNNs are not the focus of the project, this summary was rather concise. For more information, see [this](https://www.geeksforgeeks.org/deep-learning/convolutional-neural-network-cnn-in-machine-learning/) resource.$`^4`$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results\n",
    "\n",
    "**Experimental Setup:**\n",
    "\n",
    "The following steps should be taken.\n",
    "\n",
    "1) The \"BizHawk\" emulator should be downloaded.$`^{2}`$\n",
    "2) The lua \"sockets\" library should be downloaded and included in the BizHawk installation folder.$`^{5}`$ \n",
    "3) This repository should be cloned.\n",
    "4) A copy of the \"Super Mario Bros.\" ROM file should be obtained and opened in the emulator. \n",
    "5) Place mario at the start of the level you would like to train and save the game in save-slot 1.\n",
    "6) Start the `MarioServer.py` script (or run the code cell in this notebook). Wait for the \"Server Started\" notice.\n",
    "7) Open the `Mario.lua` script in the BizHawk emulator (tools -> lua console -> file -> open).\n",
    "\n",
    "The python server will then take control  of Mario and the model will begin to be trained. You can press the escape key to disconnect the client and terminate the server.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Hyperparameters:**\n",
    "\n",
    "Before you run the python server or the lua client, you can optionally change several hyperparameters. The parameters have been tuned manually, and will be refined in the future.\n",
    "\n",
    "- **`HIDDEN_LAYERS`** and **`HIDDEN_NODES`:** These parameters control the structure of the neural network.\n",
    "- **`W_*`:** These variables control how much weight is given for different actions when calculating the reward that the model has.\n",
    "  - **`W_TIME`:** How much Mario is punished for taking too long.\n",
    "  - **`W_DISTANCE`:** How much Mario is rewarded for moving to the right.\n",
    "  - **`W_DIED`:** How much Mario is punished for dying.\n",
    "  - **`W_WIN`:** How much Mario is rewarded for beating the level.\n",
    "- **`GAMMA`:** How much Mario prioritizes long term reward versues short term reward (gamma near 1 prioritizes long term gain).\n",
    "- **`EPSILON`:** The percentage of the time that the model takes a random action rather than the expected action. This allows the model to try new things.\n",
    "- **`VISION_SIZE:`** How many tiles mario can \"see\" in any direction.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "\n",
    "The primary metric is *reward*. This is the value that the model is trying to maximize. The goal of the model is to choose actions that will lead to the most reward signal in the long term. This is how that reward signal is currently calculated.\n",
    "\n",
    "``` python\n",
    "# The reward based on these parameters.\n",
    "def currentReward(playerWin, playerDied, playerX, currentFrame):\n",
    "    reward = 0\n",
    "\n",
    "    reward += (playerX / MAX_LEVEL_WIDTH) * W_DISTANCE\n",
    "    reward -= (currentFrame / MAX_LEVEL_TIME) * W_TIME\n",
    "    if playerWin: reward += W_WIN\n",
    "    if playerDied: reward -= W_DIED\n",
    "\n",
    "    return reward\n",
    "```\n",
    "\n",
    "We can see that reward is increased when Mario makes rightward progress or beats the level, and reward is decreased when Mario dies, moves to the left, or takes too long.\n",
    "\n",
    "If the reward signal is high, that indicates that the model is doing well.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Quantitative Results:**\n",
    "\n",
    "The script can be configured to export the reward value to a text file each time it is calculated. This will then be plotted to determine how the value of the reward changes over time.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Qualitative Results:**\n",
    "\n",
    "Qualatativley, we can just watch the performance of the model. A sample video is shown in section 7.2 below. We can see that the model begins very randomly and makes almost no rightward progress. After 10 minutes of gameplay, the model is able to make it much farther in the level.\n",
    "\n",
    "Parametric plots of the player's position X vs Y, will be made and displayed here in the future.\n",
    "\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE]: Server Started on 127.0.0.1:2022\n",
      "[NOTICE]: Connection made by 127.0.0.1:50867\n",
      "[NOTICE]: Connection closed by client.\n"
     ]
    }
   ],
   "source": [
    "#### EDITABLE PARAMETERS (you can change these)\n",
    "\n",
    "# The port that this python server will run on.\n",
    "PORT = 2022\n",
    "\n",
    "# The number of hidden layers and neurons in those layers of the network.\n",
    "HIDDEN_LAYERS = 2\n",
    "HIDDEN_NODES = 128\n",
    "\n",
    "# How heavily weighted each part of the reward function is.\n",
    "W_TIME = 0.1\n",
    "W_DISTANCE = 100\n",
    "W_DIED = 1000.0\n",
    "W_WIN = 100000.0\n",
    "\n",
    "\n",
    "# How much long term reward matters (near 1 to prioritize long-term gain).\n",
    "GAMMA = 0.95 \n",
    "\n",
    "# The percentage of the time that a random action will be taken (0 to use only the trained network).\n",
    "EPSILON = 0.1\n",
    "\n",
    "# The q-value we have to meet to actually take an action.\n",
    "ACTION_THRESHOLD = 0.5\n",
    "\n",
    "# CSV file to store rewards (set to None for no log file).\n",
    "REWARD_LOG_FILE = \"reward-log.csv\"\n",
    "\n",
    "\n",
    "\n",
    "#### LIBRARY IMPORTS\n",
    "\n",
    "# Standard library imports\n",
    "import os, csv, math, socket\n",
    "\n",
    "# External library imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, optimizers\n",
    "\n",
    "\n",
    "\n",
    "#### CONSTANT VARIABLES\n",
    "\n",
    "# The maximum ammount of data that the client might send at once.\n",
    "BUFFER_SIZE = 512\n",
    "\n",
    "# The maximum ammount of time between packouts without aborting (seconds).\n",
    "SOCKET_TIMEOUT_LENGTH = 10.0\n",
    "\n",
    "# The buttons that the agent can learn to press.\n",
    "BUTTON_LIST = [\"Up\", \"Down\", \"Left\", \"Right\", \"A\", \"B\"]\n",
    "\n",
    "# The width of the longest level (pixels) and time limit of the longest level (frames).\n",
    "MAX_LEVEL_WIDTH = 6656\n",
    "MAX_LEVEL_TIME = 9600\n",
    "\n",
    "\n",
    "\n",
    "#### SCRIPT VARIABLES\n",
    "\n",
    "# The current Q-Network\n",
    "qNetwork = None\n",
    "\n",
    "# The \"vision-size\" that is being used. Is determined by the length of the sent matrix.\n",
    "visionSize = None\n",
    "\n",
    "# Previous network information\n",
    "prevState = None\n",
    "prevActions = None\n",
    "\n",
    "# The number of frames that have been processed.\n",
    "currentFrame = 0\n",
    "\n",
    "# Optimizer function\n",
    "opt = optimizers.Adam(1e-4)\n",
    "\n",
    "\n",
    "\n",
    "#### UTILITY FUNCTIONS\n",
    "\n",
    "# The reward based on these parameters.\n",
    "def currentReward(playerWin, playerDied, playerX, currentFrame):\n",
    "\n",
    "    # Calculate the reward.\n",
    "    reward = 0\n",
    "    reward += (playerX / MAX_LEVEL_WIDTH) * W_DISTANCE\n",
    "    reward -= (currentFrame / MAX_LEVEL_TIME) * W_TIME\n",
    "    if playerWin: reward += W_WIN\n",
    "    if playerDied: reward -= W_DIED\n",
    "\n",
    "    # Log the reward and frame to CSV.\n",
    "    if REWARD_LOG_FILE != None:\n",
    "        with open(REWARD_LOG_FILE, \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([currentFrame, reward])\n",
    "\n",
    "    # Return the reward.\n",
    "    return reward\n",
    "\n",
    "# Creates and returns the Q-network model using TensorFlow.\n",
    "def initNetwork(inputSize):\n",
    "    # Determine the vision size from the length of the input message.\n",
    "    global visionSize\n",
    "    visionSize = math.floor((math.sqrt(inputSize-2) - 1) / 2)\n",
    "\n",
    "    # Determine the shape of the network from the vision-size.\n",
    "    stateShape = (visionSize*2+1,visionSize*2+1,1)\n",
    "\n",
    "    # Add the input layer.\n",
    "    x = layers.Input(stateShape)\n",
    "    y = layers.Flatten()(x)\n",
    "\n",
    "    # Add the hidden layers.\n",
    "    for _ in range(HIDDEN_LAYERS):\n",
    "        y = layers.Dense(HIDDEN_NODES, activation='relu')(y)\n",
    "    \n",
    "    # Add the output layer.\n",
    "    out = layers.Dense(len(BUTTON_LIST))(y)\n",
    "\n",
    "    # Return the model.\n",
    "    return Model(x, out)\n",
    "\n",
    "# Updates the Q-network model using TensorFlow.\n",
    "def updateNetwork(prevState, prevActions, reward, currentState):\n",
    "    global opt, qNetwork\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        q = qNetwork(prevState[np.newaxis,...])[0]\n",
    "        qNext = qNetwork(currentState[np.newaxis,...])[0]\n",
    "        target = tf.where(prevActions, reward + GAMMA * qNext, q)\n",
    "        loss = tf.reduce_mean((target - q)**2)\n",
    "        \n",
    "    grads = tape.gradient(loss, qNetwork.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, qNetwork.trainable_variables))\n",
    "\n",
    "# Given the state of the game, determines the best actions (buttons) to press. Called each frame.\n",
    "def processFrame(connection, data):\n",
    "    global qNetwork, prevState, prevActions, currentFrame\n",
    "\n",
    "    # Split the data into an array.\n",
    "    inputs = data.split()\n",
    "\n",
    "    # Validate the input.\n",
    "    if inputs[-1] != \"END\":\n",
    "        print(\"[WARNING]: Invalid input recieved from server.\")\n",
    "        return\n",
    "\n",
    "    # Ensure that the network exists.\n",
    "    if qNetwork == None:\n",
    "        qNetwork = initNetwork(len(inputs))\n",
    "\n",
    "    # Extract player data from the input message.\n",
    "    playerWin = int(inputs[0]) == 1\n",
    "    playerDied = int(inputs[0]) == 2\n",
    "    playerX = int(inputs[1])\n",
    "\n",
    "    # Extract level data from the input message.\n",
    "    tileMap = np.array(inputs[2:-1], dtype='int8')\n",
    "    state = tileMap.reshape((visionSize*2+1,visionSize*2+1,1))\n",
    "\n",
    "    # Take the action dictated by the network.\n",
    "    qVals = qNetwork(state[np.newaxis,...])[0].numpy()\n",
    "    actions = np.where(qVals > ACTION_THRESHOLD)[0]\n",
    "    \n",
    "    # Per-button greedy exploration. Invert each action EPSILON % of the time.\n",
    "    actions = [i for i in range(len(BUTTON_LIST)) if (i in actions) ^ (np.random.rand() < EPSILON)]\n",
    "\n",
    "    # We can't press oppisite directions at the same time in real life.\n",
    "    if (BUTTON_LIST.index(\"Up\") in actions) and (BUTTON_LIST.index(\"Down\") in actions): actions.remove(BUTTON_LIST.index(\"Down\"))\n",
    "    if (BUTTON_LIST.index(\"Left\") in actions) and (BUTTON_LIST.index(\"Right\") in actions): actions.remove(BUTTON_LIST.index(\"Left\"))\n",
    "\n",
    "\n",
    "    # Send the buttons to the network\n",
    "    actionList = [BUTTON_LIST[i] for i in actions]\n",
    "    actionList.append(\"END\\n\")\n",
    "    actionString = \" \".join(actionList)\n",
    "\n",
    "    # Send back a response.\n",
    "    connection.sendall(actionString.encode())\n",
    "\n",
    "    # Reset the frame counter if the player won or died.\n",
    "    if playerWin or playerDied:\n",
    "        currentFrame = 0\n",
    "\n",
    "    # Update the network.\n",
    "    if prevState is not None:\n",
    "        reward = currentReward(playerWin, playerDied, playerX, currentFrame)\n",
    "        prevActionsBool = np.zeros(len(BUTTON_LIST), dtype=bool)\n",
    "        prevActionsBool[prevActions] = True\n",
    "        updateNetwork(prevState, prevActionsBool, reward, state)\n",
    "    \n",
    "    # Save for the next frame.\n",
    "    prevState = state\n",
    "    prevActions = actions\n",
    "\n",
    "\n",
    "\n",
    "#### CREATE PYTHON SERVER\n",
    "\n",
    "def main():\n",
    "    global currentFrame\n",
    "\n",
    "    # Initialize CSV file with header if it doesn't exist\n",
    "    if REWARD_LOG_FILE != None:\n",
    "        if not os.path.exists(REWARD_LOG_FILE):\n",
    "            with open(REWARD_LOG_FILE, \"w\", newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\"frame\", \"reward\"])\n",
    "\n",
    "    # Create and bind the socket.\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    sock.bind(('127.0.0.1', PORT))\n",
    "    sock.listen(1)\n",
    "    print(f\"[NOTICE]: Server Started on 127.0.0.1:{PORT}\")\n",
    "\n",
    "    # Wait for a client to connect.\n",
    "    connection, clientAddress = sock.accept()\n",
    "\n",
    "    # Ensure valid connection.\n",
    "    with connection:\n",
    "        print(f\"[NOTICE]: Connection made by {clientAddress[0]}:{clientAddress[1]}\")\n",
    "\n",
    "        # After making the initial connection, set a time limit on the connection.\n",
    "        connection.settimeout(SOCKET_TIMEOUT_LENGTH) \n",
    "\n",
    "        # The main loop. Wait for a network, process it, and send back inputs.\n",
    "        while True:\n",
    "            try:\n",
    "                # Wait for an incoming message.\n",
    "                data = connection.recv(BUFFER_SIZE).decode()\n",
    "                if not data:\n",
    "                    print(\"[NOTICE]: Connection closed by client.\")\n",
    "                    break\n",
    "\n",
    "                # Advance the current frame.\n",
    "                currentFrame += 1\n",
    "\n",
    "                # Process the sent data, decide what buttons to press, and update the network.\n",
    "                processFrame(connection, data)\n",
    "\n",
    "            except socket.timeout:\n",
    "                print(\"[NOTICE]: Connection timeout. Closing connection.\")\n",
    "                break\n",
    "            except socket.error as e:\n",
    "                print(f\"[ERROR]: Socket Error: {e}\")\n",
    "                break\n",
    "            \n",
    "        connection.close()\n",
    "        sock.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following video shows the model's performance at two different times. First at t=0, then at t=10 min:\n",
    "\n",
    "<video width=\"960\" height=\"540\" controls src=\"./img/progress-1.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows agent reward as a function of time. On the x-axis is the number of minutes that the game has been running. On the y-axis is the amount of reward that the agent has received. The plot is shown until the network bets the level for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'REWARD_LOG_FILE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m SEND_INTERVAL \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Ensure that the log file exists.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mREWARD_LOG_FILE\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Load the data file.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(REWARD_LOG_FILE)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Add a data column for real world minutes.\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'REWARD_LOG_FILE' is not defined"
     ]
    }
   ],
   "source": [
    "SEND_INTERVAL = 5\n",
    "\n",
    "# Ensure that the log file exists.\n",
    "if REWARD_LOG_FILE != None:\n",
    "    # Load the data file.\n",
    "    df = pd.read_csv(REWARD_LOG_FILE)\n",
    "\n",
    "    # Add a data column for real world minutes.\n",
    "    df[\"minutes\"] = (df[\"frame\"] * SEND_INTERVAL / 60) / 60\n",
    "\n",
    "    # Plot the reward as a function of time.\n",
    "    plt.plot(df[\"minutes\"], df[\"reward\"])\n",
    "    plt.xlabel(\"Play Time [Min]\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "\n",
    "    # Show the plot.\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion\n",
    "\n",
    "**Interpretation of Results:**\n",
    "\n",
    "Observing the agent in action provides clear visual confirmation that the network is capable of learning to play the game. If we watch for long enough, the agent is even able to complete the level! This behavior is mirrored quantitatively by the upward trend in reward over time. Initially, the agent acts almost randomly, often failing to make significant progress. As training continues, it gradually discovers actions that increase its reward. However, performance remains inconsistent. Temporary drops in reward occur when the agent falls into pits or collides with enemies, interrupting its forward progress. \n",
    "\n",
    "These fluctuations indicate that while the model learned effective strategies, it has not yet achieved complete mastery of the game environment.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Error Analysis:**\n",
    "\n",
    "Qualitative observations reveal that the agent struggles in specific situations. Particularly when moving left is required to continue making progress in the level. The agent is not able to overcome the short-term punishment to the reward function necessary to find long-term success. Some failures are likely due to limitations in the state representation, which captures only a fixed number of tiles around the player (the VISION_SIZE) and may miss relevant context for precise timing or positioning. \n",
    "\n",
    "<br/>\n",
    "\n",
    "**Overfitting:**\n",
    "\n",
    "Because the agent learns directly from gameplay experiences rather than a static dataset, traditional overfitting is less of a concern. However, the model can still over-specialize to specific patterns it encounters during training, potentially reducing performance on unseen levels. Early experiments indicate that reward improvements generalize reasonably well across similar layouts, but moving the agent to drastically different levels did not show success.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "The main limitations of this project include the state representation. As a developer, I decided what parameters the network would be able to \"see\". Had I chosen to give the network more or less information, results may have changed. Additionally, the reliance on reward shaping may bias behavior toward short-term gains. \n",
    "\n",
    "The biggest limitation are the computational constraints of training over long periods. The agent will play the game in real-time or close to real-time. This means that to collect a large sample of data, the agent must train for long periods of time. \n",
    "\n",
    "Finally, because training is conducted on a single game, results will not directly transfer to other games or platforms without additional adaptation.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions & Future Work\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "This project demonstrates that a Deep Q-Learning agent can learn to play Super Mario Bros on the NES, though wether the model can achieve full mastery remains an open question. By extracting the game state from the emulator and encoding it into a compact vector, the agent was able to interact with the environment and gradually improve its performance. Both visual observations and cumulative reward trends confirm that the agent learns to make forward progress and eventually complete levels, illustrating how complex behavior can emerge from simple reward signals.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Performance:**\n",
    "\n",
    "Deep Q-Learning proved effective for this task, as its neural network approximation of the Q-function enabled the agent to generalize across diverse game states and identify action sequences that maximize long-term reward. While DQN performed well, alternative approaches such as NEAT may offer advantages in certain scenarios. Past NEAT based aproaches have also proven more effective at tackling this problem.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Future Work:**\n",
    "\n",
    "Several avenues exist to extend this work. Combining multiple trained models or “breeding” networks that have learned complementary strategies could improve performance. Training networks over longer periods and on multiple levels would enhance generalization and robustness. Additionally, automated hyperparameter optimization could replace manual tuning, potentially yielding more efficient learning and higher-quality agents.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. References\n",
    "\n",
    "1) https://en.wikipedia.org/wiki/Super_Mario_Bros\n",
    "2) https://tasvideos.org/Bizhawk\n",
    "3) https://www.researchgate.net/figure/Q-Learning-vs-Deep-Q-Learning-Difference-in-the-agents-brain-In-Q-learning-the_fig4_382675650\n",
    "4) https://www.geeksforgeeks.org/deep-learning/convolutional-neural-network-cnn-in-machine-learning/\n",
    "5) https://lunarmodules.github.io/luasocket/\n",
    "6) https://github.com/Xander-Carroll/PHYSICS5680-Neural-Network-Final\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
